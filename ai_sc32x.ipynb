{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy_9hARVUo9L"
      },
      "source": [
        "# SC32x \n",
        "## 자연어처리 (Natural Language Processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dNUXHiLbBdp"
      },
      "source": [
        "# Part 1 : 개념 요약\n",
        "\n",
        "> 다음의 키워드에 대해서 **한 줄**로 간단하게 요약해주세요. (세션 노트를 참고하여도 좋습니다.)<br/>\n",
        "> **Tip : 아래 문제를 먼저 수행한 후 모델 학습 등 시간이 오래 걸리는 셀이 실행되는 동안 아래 내용을 작성하면 시간을 절약할 수 있습니다.**\n",
        "\n",
        "**N321**\n",
        "- Stopwords(불용어)\n",
        "- Stemming과 Lemmatization\n",
        "- Bag-of-Words\n",
        "- TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***이곳에 답안을 입력해주세요***\n",
        "\n",
        "- Stopwords(불용어):\n",
        " 문서의 특성을 나타내는데에 영향을 주지 않거나 의미가 없는 단어.\n",
        "- Stemming과 Lemmatization:\n",
        " 문서의 특성을 표현하는 단어들를 정규화하는 방법.\n",
        "- Bag-of-Words:\n",
        " 문서에 사용된 단어들를 문법이나 순서를 고려하지 않고 빈도수만 고려하여 문서를 벡터화하는 방법.\n",
        "- TF-IDF:\n",
        " 해당 문서에 등장하는 단어의 빈도수와 해당 문서의 단어가 등장하는 문서의 빈도수를 이용하여 문서를 벡터화하는 방식.\n"
      ],
      "metadata": {
        "id": "NdfK-DKjF8UZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**N322**\n",
        "- Word2Vec:\n",
        "CBoW나 Skip-gram 방식으로 문서의 단어를 표현하는 임베딩 벡터를 학습시키고 임베딩 벡터로 단어를 표현하는 것.\n",
        "- fastText: 단어를 철자단위로 분석하고 임베딩 벡터를 구하여 단어를 표현하는 것."
      ],
      "metadata": {
        "id": "O2go82JnF1y4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***이곳에 답안을 입력해주세요***"
      ],
      "metadata": {
        "id": "EIwLcmK4GDxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N323**\n",
        "- RNN\n",
        "- LSTM, GRU\n",
        "- Attention"
      ],
      "metadata": {
        "id": "Oyu706gAF4L8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***이곳에 답안을 입력해주세요***\n",
        "- RNN :\n",
        "자신의 출력 값이 다시 입력으로 들어오는 인공 신경망 구조.\n",
        "- LSTM, GRU :\n",
        "LSTM은 RNN의 기울기 소실 문제를 해결하기 위해 활성화 함수를 거치지 않는 cell-state와 과거의 정보와 새로 입력된 정보를 이용할 비중을 정하는 3가지 게이트를 추가한 인공 신경망 구조. GRU는 LSTM의 cell-state가 제거되고 3가지 게이트를 간소화하여 LSTM의 전체적인 맥락을 따르는 인공신경망 구조.\n",
        "- Attention :\n",
        "모든 단어의 정보를 활용하기 위해 RNN에 들어온 단어에 대한 Hidden state 벡터들과 해당 Hidden state 벡터를 이용하여 해당 단어의 문맥 상의 집중도를 계산하는 방법."
      ],
      "metadata": {
        "id": "cWe1aBzTGFy5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g07_yjCgbBdp"
      },
      "source": [
        "# Part 2 : Fake/Real News Dataset\n",
        "\n",
        "한 주간 자연어처리 기법을 배우면서 여러분은 다양한 기술들을 접했습니다.<br/>\n",
        "어떻게 텍스트 데이터를 다뤄야 하는지, 텍스트를 벡터화 하는 법, 문서에서 토픽을 모델하는 법 등 다양한 NLP 기법을 배웠는데요.<br/>\n",
        "이번 스프린트 챌린지에선 [Fake/Real News Dataset](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset)을 사용하여 배운 것들을 복습해보는 시간을 갖겠습니다.\n",
        "\n",
        "**주의 : 모델의 성능을 최대한 끌어올리는 것이 아닌 모델 구동에 초점을 맞춰주세요.<br/>\n",
        "모든 문제를 완료한 후에도 \"시간이 남았다면\" 정확도를 올리는 것에 도전하시는 것을 추천드립니다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BCX5K0nP0ZKQ"
      },
      "outputs": [],
      "source": [
        "# 코드 실행 전 seed를 지정하겠습니다.\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C08-JiyNbdQy"
      },
      "source": [
        "## 2.0 데이터셋을 불러옵니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyndWa5hxEmk"
      },
      "source": [
        "- 위 캐글 링크에서 데이터셋을 받아 업로드 합니다.<br/>\n",
        "(직접 업로드하게 되면 시간이 꽤 걸리므로 **drive_mount** 나 **kaggle 연동**하시는 것을 추천드립니다.)\n",
        "\n",
        "- 'label' 열을 만들어 Fake = 1, True = 0 로 레이블링해줍니다.\n",
        "- 두 파일을 합쳐 하나의 데이터프레임에 저장해 준 후 데이터를 섞어줍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbxEHBanUo9d"
      },
      "source": [
        "## 2.1 TF-IDF 를 활용하여 특정 뉴스와 유사한 뉴스 검색하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4R5mLMS5c2I"
      },
      "source": [
        "시간상 특별한 **전처리 없이** 아래 태스크를 수행하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2B-6Wkk5YGD"
      },
      "source": [
        "### 2.1.1 TFidfVectorizer를 사용하여 문서-단어 행렬(Document-Term Matrix) 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0Kw9OQwmUo9e",
        "outputId": "f73e73b0-433a-4481-b6a0-76aab4ff852f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 44898 entries, 0 to 21416\n",
            "Data columns (total 5 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   title    44898 non-null  object\n",
            " 1   text     44898 non-null  object\n",
            " 2   subject  44898 non-null  object\n",
            " 3   date     44898 non-null  object\n",
            " 4   label    44898 non-null  int64 \n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 2.1+ MB\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "fake = pd.read_csv('Fake.csv')\n",
        "true = pd.read_csv('True.csv')\n",
        "fake['label'] = 1\n",
        "true['label'] = 0\n",
        "df = pd.concat([fake,true])\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfid = TfidfVectorizer(max_features = 3000)\n",
        "X = tfid.fit_transform(df['text'])\n",
        "y = df['label']"
      ],
      "metadata": {
        "id": "pyFlb_-or4Fs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeTMNUMM5WgQ"
      },
      "source": [
        "### 2.1.2 KNN 알고리즘을 사용하여 유사한 문서 검색하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lhqO6dqFkO2"
      },
      "source": [
        "- **42번 인덱스의 문서**와 가장 유사한 **5개 문서(42번 포함)의 인덱스**와 **해당 인덱스의 레이블**을 나타내주세요.\n",
        "- NN 모델의 파라미터 중 `algorithm = 'kd_tree'` 로 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8jjaQO8O6UKd",
        "outputId": "5e83f093-8fa0-49f2-f9e0-73de8c74bae9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   42 23876 36788 23864 23838]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "neigh = KNeighborsClassifier(n_neighbors=5,algorithm = 'kd_tree')\n",
        "neigh.fit(X, y)\n",
        "print(neigh.kneighbors(X = X[42],n_neighbors = 5)[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghQSPbQE7P8b"
      },
      "source": [
        "## 2.2 Keras Embedding을 사용하여 분류하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEl8iU6j8I4M"
      },
      "source": [
        "### 2.2.0 데이터셋 split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIQqkDeOFkO3"
      },
      "source": [
        "- Train, Test 데이터셋으로 분리(Split)하여 주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "55XSZyY_Sj54"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(df['text'],y,test_size = 0.2,random_state = 42 , stratify = y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1xXxSMn7fyt"
      },
      "source": [
        "### 2.2.1 단어 벡터의 평균을 이용하여 분류해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_ZsjAVg7ndG"
      },
      "source": [
        "N322에서 했던 단어 임베딩 벡터의 평균을 사용하여 문장을 분류하는 작업을 수행해봅시다.<br/>\n",
        "인스턴스마다 텍스트 길이가 길고 시간이 오래 걸리므로 시간상 epoch 수를 **10 이하**로 하는 것을 추천드립니다.<br/>\n",
        "모델 구동이 목적이므로 임베딩 차원 수를 크지 않게(50이하)로 설정해주세요.<br/>\n",
        "**권장사항 : `max_len` 은 텍스트 길이 평균보다 높게 설정해주세요.**<br/>\n",
        "\n",
        "> **Tip : 모델이 학습하는 동안 2.2.3의 내용을 작성하면 시간을 절약할 수 있습니다.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dHber3qBBwOl",
        "outputId": "a01a9477-c703-4314-a350-d2baeb2da1cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "449/449 [==============================] - 6s 11ms/step - loss: 0.6918 - acc: 0.5140 - val_loss: 0.6904 - val_acc: 0.5131\n",
            "Epoch 2/10\n",
            "449/449 [==============================] - 3s 6ms/step - loss: 0.6903 - acc: 0.5303 - val_loss: 0.6891 - val_acc: 0.5710\n",
            "Epoch 3/10\n",
            "449/449 [==============================] - 3s 6ms/step - loss: 0.6894 - acc: 0.5637 - val_loss: 0.6881 - val_acc: 0.5724\n",
            "Epoch 4/10\n",
            "449/449 [==============================] - 2s 5ms/step - loss: 0.6887 - acc: 0.5627 - val_loss: 0.6875 - val_acc: 0.5678\n",
            "Epoch 5/10\n",
            "449/449 [==============================] - 4s 10ms/step - loss: 0.6881 - acc: 0.5615 - val_loss: 0.6868 - val_acc: 0.5699\n",
            "Epoch 6/10\n",
            "449/449 [==============================] - 3s 6ms/step - loss: 0.6875 - acc: 0.5632 - val_loss: 0.6862 - val_acc: 0.5703\n",
            "Epoch 7/10\n",
            "449/449 [==============================] - 2s 6ms/step - loss: 0.6870 - acc: 0.5623 - val_loss: 0.6857 - val_acc: 0.5686\n",
            "Epoch 8/10\n",
            "449/449 [==============================] - 4s 8ms/step - loss: 0.6865 - acc: 0.5627 - val_loss: 0.6853 - val_acc: 0.5693\n",
            "Epoch 9/10\n",
            "449/449 [==============================] - 3s 6ms/step - loss: 0.6860 - acc: 0.5621 - val_loss: 0.6846 - val_acc: 0.5695\n",
            "Epoch 10/10\n",
            "449/449 [==============================] - 3s 7ms/step - loss: 0.6855 - acc: 0.5646 - val_loss: 0.6841 - val_acc: 0.5725\n",
            "281/281 [==============================] - 1s 4ms/step - loss: 0.6856 - acc: 0.5641\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6856023669242859, 0.5641425251960754]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "import spacy\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tkzer = Tokenizer(num_words = 2000)\n",
        "tkzer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_encoded= tkzer.texts_to_sequences(X_train)\n",
        "X_test_encoded = tkzer.texts_to_sequences(X_test)\n",
        "\n",
        "# print(f'학습 데이터에 있는 문서의 평균 토큰 수: {np.mean([len(sent) for sent in X_train_encoded], dtype=int)}')\n",
        "\n",
        "maxlen=400\n",
        "X_train_final = pad_sequences(X_train_encoded, maxlen=maxlen, padding='post')\n",
        "y_train=np.array(y_train)\n",
        "X_test_final = pad_sequences(X_test_encoded, maxlen=maxlen, padding='post')\n",
        "y_test=np.array(y_test)\n",
        "\n",
        "vocab_size = len(tkzer.word_index)\n",
        "\n",
        "# embedding_matrix = np.zeros((vocab_size, 50))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50,input_length=maxlen, trainable=False )) #,weights=[embedding_matrix]))\n",
        "model.add(GlobalAveragePooling1D()) # 입력되는 단어 벡터의 평균을 구하는 함수입니다.\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "model.fit(X_train_final, y_train, batch_size=64, epochs=10, validation_split=0.2)\n",
        "\n",
        "model.evaluate(X_test_final, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SzUwLkcAK1A"
      },
      "source": [
        "### 2.2.2 LSTM을 사용하여 텍스트 분류 수행해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4UZ9ZqOAIjw"
      },
      "source": [
        "N323에서 했던 단어 임베딩 벡터의 평균을 사용하여 문장을 분류하는 작업을 수행해봅시다.<br/>\n",
        "인스턴스마다 텍스트 길이가 길어 시간이 매우 오래 걸리므로 <br/>\n",
        "**층을 최소한으로 쌓고**, epoch 수를 **3 이하**로 하는 것을 추천드립니다.<br/>\n",
        "\n",
        "> **Tip : 모델이 학습하는 동안 2.2.3의 내용을 작성하면 시간을 절약할 수 있습니다.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "9PLNlzEVBSyE",
        "outputId": "20845f22-4130-4bd7-9163-eb24a62ba2d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, None, 50)          6277800   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 10)                2440      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,280,251\n",
            "Trainable params: 6,280,251\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "225/225 [==============================] - 167s 724ms/step - loss: 0.4517 - accuracy: 0.7827 - val_loss: 0.2479 - val_accuracy: 0.9195\n",
            "Epoch 2/3\n",
            "225/225 [==============================] - 154s 686ms/step - loss: 0.1935 - accuracy: 0.9455 - val_loss: 0.1702 - val_accuracy: 0.9568\n",
            "Epoch 3/3\n",
            "225/225 [==============================] - 154s 684ms/step - loss: 0.1446 - accuracy: 0.9646 - val_loss: 0.1898 - val_accuracy: 0.9493\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efc9a206560>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Embedding(vocab_size, 50), \n",
        "  tf.keras.layers.LSTM(10, dropout=0.2, recurrent_dropout=0.2), \n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')  \n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='RMSprop', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "callback = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=3)\n",
        "model.fit(X_train_final,y_train,batch_size = 128, epochs = 3 , validation_split=0.2,callbacks = [callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test_final, y_test)"
      ],
      "metadata": {
        "id": "m7aWqrBW2s3s",
        "outputId": "422f3000-10c1-4dd6-b2fb-14e12627d710",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "281/281 [==============================] - 15s 54ms/step - loss: 0.1904 - accuracy: 0.9492\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.19041146337985992, 0.9492204785346985]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsNOs2wmBV1z"
      },
      "source": [
        "### 2.2.3 위에서 실행한 내용에 대해 다시 알아봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMR2t2wOEPYm"
      },
      "source": [
        "#### a) 데이터셋을 학습할 때 사용하는 `pad_sequences`  메서드에 대해 설명해주세요.<br/>어떤 기능을 하나요? 모델을 학습할 때 왜 필요한가요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7_ECWSYQ4JV"
      },
      "source": [
        "***이곳에 답안을 입력해주세요***\n",
        "문서별로 단어의 수가 다른데 모델에 입력되기 위해서는 모든 문서가 동일한 shape이여야 하기 때문에 pad_sequences가 필요하다.\n",
        "pad_sequences는 모든 토큰화된 문서들의 길이를, 기준 값보다 짧을 경우 padding를, 기준 값보다 큰 경우 slicing하여 모두 같은 길이로 전처리해준다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeZ6AcloEYD5"
      },
      "source": [
        "#### b) 2.2.1과 2.2.2에서 사용한 각 모델의 evaluation 성능은 어떻게 나왔나요?<br/>각 모델의 장단점은 무엇이라고 생각하나요?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SyqQzzHQ4EP"
      },
      "source": [
        "***이곳에 답안을 입력해주세요***\n",
        "\n",
        "2.2.2의 평가 성능이 2.2.1보다 더 좋은 것으로 나타났다.\n",
        "2.2.1 모델은 간단하고 학습시간이 짧은 장점이 있지만 정확도가 높지 않다.\n",
        "2.2.2 모델은 정확도 및 성능이 높지만 학습시간이 길고 모델이 복잡하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUwKojAGM82f"
      },
      "source": [
        "#### c) 종래의 RNN(Recurrent Neural Networks) 대신 LSTM(Long-Short Term Memory)을 사용하는 이유는 무엇인가요?<br/>(i.e. RNN에 비해 LSTM의 좋은 점을 설명해주세요.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y44XfugQ_HR"
      },
      "source": [
        "***이곳에 답안을 입력해주세요***\n",
        "\n",
        "LSTM은 RNN에 비해 기울기 소실 또는 폭주 문제가 없다. 즉 긴 길이의 문서를 더 잘 학습한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J1kzTmDEaLU"
      },
      "source": [
        "#### d) LSTM이나 RNN을 사용하는 예시를 **3개**이상 제시하고 해당되는 경우에 왜 LSTM이나 RNN을 사용하는 것 적절한지 간단하게 설명해주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck8GPjc_Q_vA"
      },
      "source": [
        "***이곳에 답안을 입력해주세요***\n",
        "RNN: 댓글 감정분류,스팸 메시지 분류. 비교적 짧은 길이의 문장들로 RNN으로 적절히 분류할 수 있다. \n",
        "LSTM: 주식 그래프 예측. LSTM은 긴 길이의 시계열 데이터를 잘 처리할 수 있어서 주식 그래프와 같은 데이터를 잘 학습할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4pP7g8DE0Cz"
      },
      "source": [
        "#### e) 이외에 N324 에서 배운 자연어처리 모델과 관련된 키워드를 3개 이상 적어주세요. <br/> (해당 키워드에 대한 설명은 옵션입니다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-sO4mMuRAhp"
      },
      "source": [
        "***이곳에 답안을 입력해주세요***\n",
        "Transformer, Positional encoding, Self attention, Multi head attention, Masked self attention, Encoder-Decoder Attention "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRn44qjhUo9j"
      },
      "source": [
        "# Advanced Goals: 3점을 획득하기 위해선 아래의 조건 중 하나 이상을 만족해야합니다\n",
        " \n",
        "- 2.1 에서 TF-IDF(`TfidfVectorizer`)가 아닌 방법을 사용하여 유사도 검색을 수행해보세요.<br/>\n",
        "TF-IDF와 해당 방법의 차이를 설명해주세요. \n",
        "- 2.2 에서 사용한 방법을 재사용하되 하이퍼 파라미터를 조정하거나 모델 구조를 변경하여 성능을 올려봅시다.<br/>**(주의 : GridSearch, RandomSearch 등의 방법을 사용하여도 좋으나 시간이 오래 걸리므로 범위를 잘 선택해야 합니다.)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz2uaXFYUo9j"
      },
      "outputs": [],
      "source": [
        "# 이 곳에 답안을 작성하시길 바랍니다"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernel_info": {
      "name": "u4-s1-nlp"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "nteract": {
      "version": "0.22.4"
    },
    "toc-autonumbering": false
  },
  "nbformat": 4,
  "nbformat_minor": 0
}